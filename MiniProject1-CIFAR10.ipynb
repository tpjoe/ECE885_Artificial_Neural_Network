{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal 1: Following example from Keras demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Flatten, Activation\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "from keras import backend as K\n",
    "from keras.callbacks import EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import GridSearchCV  \n",
    "import pickle\n",
    "import seaborn as sns; sns.set()\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 32, 32, 3)\n",
      "50000 train samples\n",
      "10000 test samples\n",
      "Using real-time data augmentation.\n",
      "Epoch 1/25\n",
      "1563/1563 [==============================] - 232s 148ms/step - loss: 1.8719 - acc: 0.3149 - val_loss: 1.5580 - val_acc: 0.4327\n",
      "Epoch 2/25\n",
      "1563/1563 [==============================] - 234s 150ms/step - loss: 1.5567 - acc: 0.4320 - val_loss: 1.3440 - val_acc: 0.5179\n",
      "Epoch 3/25\n",
      "1563/1563 [==============================] - 241s 154ms/step - loss: 1.4341 - acc: 0.4843 - val_loss: 1.2413 - val_acc: 0.5606\n",
      "Epoch 4/25\n",
      "1563/1563 [==============================] - 238s 152ms/step - loss: 1.3361 - acc: 0.5220 - val_loss: 1.2029 - val_acc: 0.5717\n",
      "Epoch 5/25\n",
      "1563/1563 [==============================] - 238s 152ms/step - loss: 1.2630 - acc: 0.5495 - val_loss: 1.1410 - val_acc: 0.5978\n",
      "Epoch 6/25\n",
      "1563/1563 [==============================] - 234s 150ms/step - loss: 1.1982 - acc: 0.5729 - val_loss: 1.0290 - val_acc: 0.6416\n",
      "Epoch 7/25\n",
      "1563/1563 [==============================] - 233s 149ms/step - loss: 1.1452 - acc: 0.5951 - val_loss: 0.9636 - val_acc: 0.6643\n",
      "Epoch 8/25\n",
      "1563/1563 [==============================] - 248s 159ms/step - loss: 1.1037 - acc: 0.6081 - val_loss: 0.9344 - val_acc: 0.6734\n",
      "Epoch 9/25\n",
      "1563/1563 [==============================] - 247s 158ms/step - loss: 1.0578 - acc: 0.6243 - val_loss: 0.9070 - val_acc: 0.6796\n",
      "Epoch 10/25\n",
      "1563/1563 [==============================] - 255s 163ms/step - loss: 1.0271 - acc: 0.6389 - val_loss: 0.8569 - val_acc: 0.6982\n",
      "Epoch 11/25\n",
      "1563/1563 [==============================] - 232s 148ms/step - loss: 0.9980 - acc: 0.6506 - val_loss: 0.8800 - val_acc: 0.6932\n",
      "Epoch 12/25\n",
      "1563/1563 [==============================] - 234s 150ms/step - loss: 0.9737 - acc: 0.6587 - val_loss: 0.8383 - val_acc: 0.7082\n",
      "Epoch 13/25\n",
      "1563/1563 [==============================] - 237s 152ms/step - loss: 0.9486 - acc: 0.6663 - val_loss: 0.8096 - val_acc: 0.7152\n",
      "Epoch 14/25\n",
      "1563/1563 [==============================] - 244s 156ms/step - loss: 0.9303 - acc: 0.6743 - val_loss: 0.7659 - val_acc: 0.7355\n",
      "Epoch 15/25\n",
      "1563/1563 [==============================] - 236s 151ms/step - loss: 0.9137 - acc: 0.6795 - val_loss: 0.7904 - val_acc: 0.7250\n",
      "Epoch 16/25\n",
      "1563/1563 [==============================] - 237s 152ms/step - loss: 0.8954 - acc: 0.6880 - val_loss: 0.7597 - val_acc: 0.7365\n",
      "Epoch 17/25\n",
      "1563/1563 [==============================] - 234s 150ms/step - loss: 0.8871 - acc: 0.6921 - val_loss: 0.7510 - val_acc: 0.7419\n",
      "Epoch 18/25\n",
      "1563/1563 [==============================] - 240s 154ms/step - loss: 0.8719 - acc: 0.6958 - val_loss: 0.7607 - val_acc: 0.7369\n",
      "Epoch 19/25\n",
      "1563/1563 [==============================] - 250s 160ms/step - loss: 0.8585 - acc: 0.7014 - val_loss: 0.7287 - val_acc: 0.7502\n",
      "Epoch 20/25\n",
      "1563/1563 [==============================] - 249s 160ms/step - loss: 0.8476 - acc: 0.7045 - val_loss: 0.7166 - val_acc: 0.7532\n",
      "Epoch 21/25\n",
      "1563/1563 [==============================] - 243s 156ms/step - loss: 0.8400 - acc: 0.7084 - val_loss: 0.7122 - val_acc: 0.7536\n",
      "Epoch 22/25\n",
      "1563/1563 [==============================] - 246s 157ms/step - loss: 0.8353 - acc: 0.7087 - val_loss: 0.6978 - val_acc: 0.7614\n",
      "Epoch 23/25\n",
      "1563/1563 [==============================] - 238s 152ms/step - loss: 0.8199 - acc: 0.7136 - val_loss: 0.7108 - val_acc: 0.7557\n",
      "Epoch 24/25\n",
      "1563/1563 [==============================] - 247s 158ms/step - loss: 0.8105 - acc: 0.7180 - val_loss: 0.7115 - val_acc: 0.7553\n",
      "Epoch 25/25\n",
      "1563/1563 [==============================] - 236s 151ms/step - loss: 0.8093 - acc: 0.7183 - val_loss: 0.6856 - val_acc: 0.7660\n",
      "Saved trained model at D:\\Google_Drive\\Machine_Learning\\ECE885_Artificial_neural_network\\saved_models\\keras_cifar10_trained_model.h5 \n",
      "10000/10000 [==============================] - 13s 1ms/step\n",
      "Test loss: 0.685594499397\n",
      "Test accuracy: 0.766\n"
     ]
    }
   ],
   "source": [
    "'''Train a simple deep CNN on the CIFAR10 small images dataset.\n",
    "It gets to 75% validation accuracy in 25 epochs, and 79% after 50 epochs.\n",
    "(it's still underfitting at that point, though).\n",
    "'''\n",
    "\n",
    "np.random.seed(9)\n",
    "\n",
    "batch_size = 32\n",
    "num_classes = 10\n",
    "epochs = 25\n",
    "data_augmentation = True\n",
    "num_predictions = 20\n",
    "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "model_name = 'keras_cifar10_trained_model.h5'\n",
    "\n",
    "# The data, shuffled and split between train and test sets:\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# Convert class vectors to binary class matrices.\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), padding='same',\n",
    "                 input_shape=x_train.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# initiate RMSprop optimizer\n",
    "opt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)\n",
    "\n",
    "# Let's train the model using RMSprop\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "if not data_augmentation:\n",
    "    print('Not using data augmentation.')\n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_data=(x_test, y_test),\n",
    "              shuffle=True)\n",
    "else:\n",
    "    print('Using real-time data augmentation.')\n",
    "    # This will do preprocessing and realtime data augmentation:\n",
    "    datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip=True,  # randomly flip images\n",
    "        vertical_flip=False)  # randomly flip images\n",
    "\n",
    "    # Compute quantities required for feature-wise normalization\n",
    "    # (std, mean, and principal components if ZCA whitening is applied).\n",
    "    datagen.fit(x_train)\n",
    "\n",
    "    # Fit the model on the batches generated by datagen.flow().\n",
    "    model.fit_generator(datagen.flow(x_train, y_train,\n",
    "                                     batch_size=batch_size),\n",
    "                        epochs=epochs,\n",
    "                        validation_data=(x_test, y_test),\n",
    "                        workers=4)\n",
    "\n",
    "# Save model and weights\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "model.save(model_path)\n",
    "print('Saved trained model at %s ' % model_path)\n",
    "\n",
    "# Score trained model.\n",
    "scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2: Improving the accuracy beyond the example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_57 (Conv2D)           (None, 32, 32, 64)        1792      \n",
      "_________________________________________________________________\n",
      "batch_normalization_54 (Batc (None, 32, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_58 (Conv2D)           (None, 30, 30, 48)        27696     \n",
      "_________________________________________________________________\n",
      "batch_normalization_55 (Batc (None, 30, 30, 48)        192       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_29 (MaxPooling (None, 15, 15, 48)        0         \n",
      "_________________________________________________________________\n",
      "dropout_40 (Dropout)         (None, 15, 15, 48)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_59 (Conv2D)           (None, 15, 15, 96)        41568     \n",
      "_________________________________________________________________\n",
      "batch_normalization_56 (Batc (None, 15, 15, 96)        384       \n",
      "_________________________________________________________________\n",
      "conv2d_60 (Conv2D)           (None, 13, 13, 96)        83040     \n",
      "_________________________________________________________________\n",
      "batch_normalization_57 (Batc (None, 13, 13, 96)        384       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_30 (MaxPooling (None, 6, 6, 96)          0         \n",
      "_________________________________________________________________\n",
      "dropout_41 (Dropout)         (None, 6, 6, 96)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_61 (Conv2D)           (None, 6, 6, 192)         166080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_58 (Batc (None, 6, 6, 192)         768       \n",
      "_________________________________________________________________\n",
      "conv2d_62 (Conv2D)           (None, 4, 4, 192)         331968    \n",
      "_________________________________________________________________\n",
      "batch_normalization_59 (Batc (None, 4, 4, 192)         768       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_31 (MaxPooling (None, 2, 2, 192)         0         \n",
      "_________________________________________________________________\n",
      "dropout_42 (Dropout)         (None, 2, 2, 192)         0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 512)               393728    \n",
      "_________________________________________________________________\n",
      "batch_normalization_60 (Batc (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dropout_43 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_44 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 1,184,570\n",
      "Trainable params: 1,182,170\n",
      "Non-trainable params: 2,400\n",
      "_________________________________________________________________\n",
      "Using real-time data augmentation.\n",
      "Beginning model.\n",
      "Epoch 1/25\n",
      "1563/1563 [==============================] - 946s 605ms/step - loss: 1.8173 - acc: 0.3890 - val_loss: 1.3605 - val_acc: 0.5210\n",
      "Epoch 2/25\n",
      "1563/1563 [==============================] - 949s 607ms/step - loss: 1.1965 - acc: 0.5801 - val_loss: 1.0246 - val_acc: 0.6324\n",
      "Epoch 3/25\n",
      "1563/1563 [==============================] - 1024s 655ms/step - loss: 1.0144 - acc: 0.6504 - val_loss: 0.8473 - val_acc: 0.7068\n",
      "Epoch 4/25\n",
      "1563/1563 [==============================] - 1023s 654ms/step - loss: 0.9123 - acc: 0.6932 - val_loss: 0.7765 - val_acc: 0.7367\n",
      "Epoch 5/25\n",
      "1563/1563 [==============================] - 1012s 647ms/step - loss: 0.8315 - acc: 0.7218 - val_loss: 0.7026 - val_acc: 0.7597\n",
      "Epoch 6/25\n",
      "1563/1563 [==============================] - 1023s 654ms/step - loss: 0.7794 - acc: 0.7416 - val_loss: 0.6765 - val_acc: 0.7734\n",
      "Epoch 7/25\n",
      "1563/1563 [==============================] - 1086s 695ms/step - loss: 0.7217 - acc: 0.7610 - val_loss: 0.5923 - val_acc: 0.8012\n",
      "Epoch 8/25\n",
      "1563/1563 [==============================] - 982s 628ms/step - loss: 0.6927 - acc: 0.7734 - val_loss: 0.7263 - val_acc: 0.7563\n",
      "Epoch 9/25\n",
      "1563/1563 [==============================] - 968s 619ms/step - loss: 0.6584 - acc: 0.7841 - val_loss: 0.5766 - val_acc: 0.8066\n",
      "Epoch 10/25\n",
      "1563/1563 [==============================] - 962s 615ms/step - loss: 0.6220 - acc: 0.7956 - val_loss: 0.5228 - val_acc: 0.8192\n",
      "Epoch 11/25\n",
      "1563/1563 [==============================] - 962s 615ms/step - loss: 0.6002 - acc: 0.8031 - val_loss: 0.5109 - val_acc: 0.8282\n",
      "Epoch 12/25\n",
      "1563/1563 [==============================] - 961s 615ms/step - loss: 0.5869 - acc: 0.8088 - val_loss: 0.6184 - val_acc: 0.7906\n",
      "Epoch 13/25\n",
      "1563/1563 [==============================] - 961s 615ms/step - loss: 0.5605 - acc: 0.8153 - val_loss: 0.4540 - val_acc: 0.8471\n",
      "Epoch 14/25\n",
      "1563/1563 [==============================] - 961s 615ms/step - loss: 0.5422 - acc: 0.8238 - val_loss: 0.5187 - val_acc: 0.8332\n",
      "Epoch 15/25\n",
      "1563/1563 [==============================] - 958s 613ms/step - loss: 0.5265 - acc: 0.8260 - val_loss: 0.4551 - val_acc: 0.8435\n",
      "Epoch 16/25\n",
      "1563/1563 [==============================] - 955s 611ms/step - loss: 0.5128 - acc: 0.8317 - val_loss: 0.4168 - val_acc: 0.8599\n",
      "Epoch 17/25\n",
      "1563/1563 [==============================] - 957s 612ms/step - loss: 0.4967 - acc: 0.8370 - val_loss: 0.4920 - val_acc: 0.8322\n",
      "Epoch 18/25\n",
      "1563/1563 [==============================] - 955s 611ms/step - loss: 0.4912 - acc: 0.8399 - val_loss: 0.4676 - val_acc: 0.8419\n",
      "Epoch 19/25\n",
      "1563/1563 [==============================] - 955s 611ms/step - loss: 0.4767 - acc: 0.8449 - val_loss: 0.3961 - val_acc: 0.8638\n",
      "Epoch 20/25\n",
      "1563/1563 [==============================] - 955s 611ms/step - loss: 0.4618 - acc: 0.8471 - val_loss: 0.3887 - val_acc: 0.8673\n",
      "Epoch 21/25\n",
      "1563/1563 [==============================] - 956s 611ms/step - loss: 0.4493 - acc: 0.8521 - val_loss: 0.4123 - val_acc: 0.8642\n",
      "Epoch 22/25\n",
      "1563/1563 [==============================] - 956s 612ms/step - loss: 0.4416 - acc: 0.8539 - val_loss: 0.3837 - val_acc: 0.8719\n",
      "Epoch 23/25\n",
      "1563/1563 [==============================] - 955s 611ms/step - loss: 0.4369 - acc: 0.8566 - val_loss: 0.3869 - val_acc: 0.8692\n",
      "Epoch 24/25\n",
      "1563/1563 [==============================] - 959s 613ms/step - loss: 0.4246 - acc: 0.8598 - val_loss: 0.3945 - val_acc: 0.8684\n",
      "Epoch 25/25\n",
      "1563/1563 [==============================] - 956s 612ms/step - loss: 0.4173 - acc: 0.8625 - val_loss: 0.3757 - val_acc: 0.8753\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(64, (3, 3), padding='same', activation='relu', input_shape=x_train.shape[1:]))\n",
    "model.add(BatchNormalization(axis=-1))\n",
    "model.add(Conv2D(48, (3, 3), activation='relu'))\n",
    "model.add(BatchNormalization(axis=-1))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.20))\n",
    "model.add(Conv2D(96, (3, 3), activation='relu', padding='same'))\n",
    "model.add(BatchNormalization(axis=-1))\n",
    "model.add(Conv2D(96, (3, 3), activation='relu'))\n",
    "model.add(BatchNormalization(axis=-1))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv2D(192, (3, 3), activation='relu', padding='same'))\n",
    "model.add(BatchNormalization(axis=-1))\n",
    "model.add(Conv2D(192, (3, 3), activation='relu'))\n",
    "model.add(BatchNormalization(axis=-1))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.20))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(BatchNormalization(axis=-1))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "\n",
    "if not data_augmentation:\n",
    "    print('Not using data augmentation.')\n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_data=(x_test, y_test),\n",
    "              shuffle=True)\n",
    "else:\n",
    "    print('Using real-time data augmentation.')\n",
    "    # This will do preprocessing and realtime data augmentation:\n",
    "    datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip=True,  # randomly flip images\n",
    "        vertical_flip=False)  # randomly flip images\n",
    "\n",
    "    # Compute quantities required for feature-wise normalization\n",
    "    # (std, mean, and principal components if ZCA whitening is applied).\n",
    "    datagen.fit(x_train)\n",
    "    print('Beginning model.')\n",
    "    # Fit the model on the batches generated by datagen.flow().\n",
    "    model.fit_generator(datagen.flow(x_train, y_train,\n",
    "                                     batch_size=batch_size),\n",
    "                        epochs=epochs,\n",
    "                        validation_data=(x_test, y_test),\n",
    "                        workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confirming accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 124s 12ms/step\n",
      "Test loss: 0.375709511614\n",
      "Test accuracy: 0.8753\n"
     ]
    }
   ],
   "source": [
    "# Score trained model.\n",
    "scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Saving data\n",
    "with open( \"CIFAR10.pickle\", \"wb\" ) as file:\n",
    "    pickle.dump(( intermediate_output, intermediate_output_test), file)\n",
    "#Loading data\n",
    "with open( \"CIFAR10.pickle\", \"rb\" ) as file:\n",
    "    (intermediate_output, intermediate_output_test) = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal 3: Mofifying the learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling model model 2.\n",
      "Using real-time data augmentation.\n",
      "Beginning model.\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/25\n",
      "49984/50000 [============================>.] - ETA: 0s - loss: 1.7920 - acc: 0.4039       Learning rate:  0.001\n",
      "50000/50000 [==============================] - 963s 19ms/step - loss: 1.7920 - acc: 0.4039 - val_loss: 1.3478 - val_acc: 0.5329\n",
      "Epoch 2/25\n",
      "49984/50000 [============================>.] - ETA: 0s - loss: 1.1141 - acc: 0.6103       Learning rate:  0.000974988\n",
      "50000/50000 [==============================] - 876s 18ms/step - loss: 1.1141 - acc: 0.6103 - val_loss: 0.9388 - val_acc: 0.6746\n",
      "Epoch 3/25\n",
      "49984/50000 [============================>.] - ETA: 0s - loss: 0.8493 - acc: 0.7089       Learning rate:  0.000494993\n",
      "50000/50000 [==============================] - 860s 17ms/step - loss: 0.8493 - acc: 0.7089 - val_loss: 0.7255 - val_acc: 0.7477\n",
      "Epoch 4/25\n",
      "49984/50000 [============================>.] - ETA: 0s - loss: 0.7168 - acc: 0.7546       Learning rate:  0.00037983\n",
      "50000/50000 [==============================] - 878s 18ms/step - loss: 0.7169 - acc: 0.7546 - val_loss: 0.6345 - val_acc: 0.7783\n",
      "Epoch 5/25\n",
      "49984/50000 [============================>.] - ETA: 0s - loss: 0.6236 - acc: 0.7888       Learning rate:  0.000332745\n",
      "50000/50000 [==============================] - 882s 18ms/step - loss: 0.6235 - acc: 0.7889 - val_loss: 0.6047 - val_acc: 0.7909\n",
      "Epoch 6/25\n",
      "49984/50000 [============================>.] - ETA: 0s - loss: 0.5671 - acc: 0.8063       Learning rate:  0.000303061\n",
      "50000/50000 [==============================] - 888s 18ms/step - loss: 0.5670 - acc: 0.8064 - val_loss: 0.5196 - val_acc: 0.8197\n",
      "Epoch 7/25\n",
      "49984/50000 [============================>.] - ETA: 0s - loss: 0.5099 - acc: 0.8255       Learning rate:  0.000286406\n",
      "50000/50000 [==============================] - 892s 18ms/step - loss: 0.5099 - acc: 0.8255 - val_loss: 0.5773 - val_acc: 0.8023\n",
      "Epoch 8/25\n",
      "49984/50000 [============================>.] - ETA: 0s - loss: 0.4662 - acc: 0.8413       Learning rate:  0.000270525\n",
      "50000/50000 [==============================] - 890s 18ms/step - loss: 0.4662 - acc: 0.8413 - val_loss: 0.5257 - val_acc: 0.8271\n",
      "Epoch 9/25\n",
      "49984/50000 [============================>.] - ETA: 0s - loss: 0.4249 - acc: 0.8546       Learning rate:  0.000258956\n",
      "50000/50000 [==============================] - 891s 18ms/step - loss: 0.4249 - acc: 0.8546 - val_loss: 0.5022 - val_acc: 0.8361\n",
      "Epoch 10/25\n",
      "49984/50000 [============================>.] - ETA: 0s - loss: 0.3901 - acc: 0.8684       Learning rate:  0.000248486\n",
      "50000/50000 [==============================] - 891s 18ms/step - loss: 0.3901 - acc: 0.8684 - val_loss: 0.4680 - val_acc: 0.8452\n",
      "Epoch 11/25\n",
      "49984/50000 [============================>.] - ETA: 0s - loss: 0.3649 - acc: 0.8750       Learning rate:  0.000239975\n",
      "50000/50000 [==============================] - 901s 18ms/step - loss: 0.3650 - acc: 0.8750 - val_loss: 0.5013 - val_acc: 0.8355\n",
      "Epoch 12/25\n",
      "49984/50000 [============================>.] - ETA: 0s - loss: 0.3427 - acc: 0.8813       Learning rate:  0.000234035\n",
      "50000/50000 [==============================] - 878s 18ms/step - loss: 0.3429 - acc: 0.8813 - val_loss: 0.4712 - val_acc: 0.8465\n",
      "Epoch 13/25\n",
      "49984/50000 [============================>.] - ETA: 0s - loss: 0.3151 - acc: 0.8909       Learning rate:  0.000228906\n",
      "50000/50000 [==============================] - 877s 18ms/step - loss: 0.3151 - acc: 0.8909 - val_loss: 0.4707 - val_acc: 0.8554\n",
      "Epoch 14/25\n",
      "49984/50000 [============================>.] - ETA: 0s - loss: 0.2967 - acc: 0.8973       Learning rate:  0.000222649\n",
      "50000/50000 [==============================] - 913s 18ms/step - loss: 0.2967 - acc: 0.8973 - val_loss: 0.4913 - val_acc: 0.8496\n",
      "Epoch 15/25\n",
      "49984/50000 [============================>.] - ETA: 0s - loss: 0.2822 - acc: 0.9035       Learning rate:  0.000218572\n",
      "50000/50000 [==============================] - 905s 18ms/step - loss: 0.2821 - acc: 0.9036 - val_loss: 0.4928 - val_acc: 0.8497\n",
      "Epoch 16/25\n",
      "49984/50000 [============================>.] - ETA: 0s - loss: 0.2672 - acc: 0.9088       Learning rate:  0.000215415\n",
      "50000/50000 [==============================] - 911s 18ms/step - loss: 0.2672 - acc: 0.9088 - val_loss: 0.4932 - val_acc: 0.8501\n",
      "Epoch 17/25\n",
      "49984/50000 [============================>.] - ETA: 0s - loss: 0.2550 - acc: 0.9136       Learning rate:  0.000212231\n",
      "50000/50000 [==============================] - 919s 18ms/step - loss: 0.2550 - acc: 0.9136 - val_loss: 0.4845 - val_acc: 0.8515\n",
      "Epoch 18/25\n",
      "49984/50000 [============================>.] - ETA: 0s - loss: 0.2375 - acc: 0.9185       Learning rate:  0.000209644\n",
      "50000/50000 [==============================] - 918s 18ms/step - loss: 0.2375 - acc: 0.9185 - val_loss: 0.5005 - val_acc: 0.8555\n",
      "Epoch 19/25\n",
      "49984/50000 [============================>.] - ETA: 0s - loss: 0.2216 - acc: 0.9219       Learning rate:  0.000206019\n",
      "50000/50000 [==============================] - 911s 18ms/step - loss: 0.2217 - acc: 0.9219 - val_loss: 0.5071 - val_acc: 0.8510\n",
      "Epoch 20/25\n",
      "49984/50000 [============================>.] - ETA: 0s - loss: 0.2142 - acc: 0.9263       Learning rate:  0.00020278\n",
      "50000/50000 [==============================] - 925s 18ms/step - loss: 0.2141 - acc: 0.9263 - val_loss: 0.5166 - val_acc: 0.8520\n",
      "Epoch 21/25\n",
      "49984/50000 [============================>.] - ETA: 0s - loss: 0.2083 - acc: 0.9290       Learning rate:  0.000201253\n",
      "50000/50000 [==============================] - 901s 18ms/step - loss: 0.2084 - acc: 0.9290 - val_loss: 0.5257 - val_acc: 0.8513\n",
      "Epoch 22/25\n",
      "49984/50000 [============================>.] - ETA: 0s - loss: 0.1995 - acc: 0.9311       Learning rate:  0.000200115\n",
      "50000/50000 [==============================] - 900s 18ms/step - loss: 0.1994 - acc: 0.9311 - val_loss: 0.5049 - val_acc: 0.8564\n",
      "Epoch 23/25\n",
      "49984/50000 [============================>.] - ETA: 0s - loss: 0.1909 - acc: 0.9338       Learning rate:  0.000198322\n",
      "50000/50000 [==============================] - 903s 18ms/step - loss: 0.1910 - acc: 0.9337 - val_loss: 0.5077 - val_acc: 0.8582\n",
      "Epoch 24/25\n",
      "49984/50000 [============================>.] - ETA: 0s - loss: 0.1780 - acc: 0.9388       Learning rate:  0.00019665\n",
      "50000/50000 [==============================] - 926s 19ms/step - loss: 0.1781 - acc: 0.9387 - val_loss: 0.5306 - val_acc: 0.8547\n",
      "Epoch 25/25\n",
      "49984/50000 [============================>.] - ETA: 0s - loss: 0.1762 - acc: 0.9392       Learning rate:  0.000194136\n",
      "50000/50000 [==============================] - 968s 19ms/step - loss: 0.1763 - acc: 0.9392 - val_loss: 0.5351 - val_acc: 0.8520\n"
     ]
    }
   ],
   "source": [
    "# define learning rate function\n",
    "\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "\n",
    "class LossHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        #sd.append(step_decay(len(self.losses)))\n",
    "        print('       Learning rate: ', K.eval(self.model.optimizer.lr))\n",
    "        #print('       Loss Check: ', self.losses[-1])\n",
    "\n",
    "\n",
    "def scheduler(epoch):\n",
    "    if epoch > 0:\n",
    "        eta0 = 0.001/np.exp(1.8173) #Using the previous model's initial loss, 0.001 is the default lr vlue for Adam.\n",
    "        eta1 = eta0*np.exp(history.losses[-1])\n",
    "    else:\n",
    "        eta1 = 0.001\n",
    "    return eta1\n",
    "\n",
    "print('Compiling model {}.'.format('model 2'))\n",
    "\n",
    "model_2 = Sequential()\n",
    "model_2.add(Conv2D(64, (3, 3), padding='same', activation='relu', input_shape=x_train.shape[1:]))\n",
    "model_2.add(BatchNormalization(axis=-1))\n",
    "model_2.add(Conv2D(48, (3, 3), activation='relu'))\n",
    "model_2.add(BatchNormalization(axis=-1))\n",
    "model_2.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model_2.add(Dropout(0.20))\n",
    "model_2.add(Conv2D(96, (3, 3), activation='relu', padding='same'))\n",
    "model_2.add(BatchNormalization(axis=-1))\n",
    "model_2.add(Conv2D(96, (3, 3), activation='relu'))\n",
    "model_2.add(BatchNormalization(axis=-1))\n",
    "model_2.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model_2.add(Dropout(0.25))\n",
    "model_2.add(Conv2D(192, (3, 3), activation='relu', padding='same'))\n",
    "model_2.add(BatchNormalization(axis=-1))\n",
    "model_2.add(Conv2D(192, (3, 3), activation='relu'))\n",
    "model_2.add(BatchNormalization(axis=-1))\n",
    "model_2.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model_2.add(Dropout(0.20))\n",
    "model_2.add(Flatten())\n",
    "model_2.add(Dense(512, activation='relu'))\n",
    "model_2.add(BatchNormalization(axis=-1))\n",
    "model_2.add(Dropout(0.5))\n",
    "model_2.add(Dense(256, activation='relu'))\n",
    "model_2.add(Dropout(0.5))\n",
    "model_2.add(Dense(num_classes, activation='softmax'))\n",
    "model_2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "if not data_augmentation:\n",
    "    print('Not using data augmentation.')\n",
    "    model_2.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_data=(x_test, y_test),\n",
    "              shuffle=True)\n",
    "else:\n",
    "    print('Using real-time data augmentation.')\n",
    "    # This will do preprocessing and realtime data augmentation:\n",
    "    datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip=True,  # randomly flip images\n",
    "        vertical_flip=False)  # randomly flip images\n",
    "\n",
    "    # Compute quantities required for feature-wise normalization\n",
    "    # (std, mean, and principal components if ZCA whitening is applied).\n",
    "    datagen.fit(x_train)\n",
    "    print('Beginning model.')\n",
    "    # Fit the model on the batches generated by datagen.flow().\n",
    "    lrate=LearningRateScheduler(scheduler)   \n",
    "    history = LossHistory()\n",
    "    history_model2 = model_2.fit(x_train, y_train, batch_size=batch_size, epochs=25, verbose=1, \\\n",
    "                             validation_data=(x_test, y_test), callbacks=[history, lrate])\n",
    "\n",
    "#print(history.losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 61s 6ms/step\n",
      "Test loss: 0.535131616455\n",
      "Test accuracy: 0.852\n"
     ]
    }
   ],
   "source": [
    "# Score trained model.\n",
    "scores = model_2.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
