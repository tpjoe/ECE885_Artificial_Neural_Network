{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary of both works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|   <font size=4>          Topics            |         |<font size=4> MNIST Dataset                 |<font size=4> CIFAR10 Dataset               |\n",
    "|:------------------------------|---------|:-------------------------------|:------------------------------|\n",
    "| **Test set accuracy of Keras demos** |         | 98.89% [12 epochs]   | 76.60% [25 epochs]          |\n",
    "| **Description of my architecture** |        | **ConV (4L) --> [ FC (1Lx512) ] --> SVM**   <br> <font color=blue> Steps for training this model: </font> <br> 1. ConV and FC were trained <br> 2. Extract trained ConV weights and remove FC <br> 3. Train SVM model using flattenned output <br>   from trained ConV as an input. <br><br> Note: accuracy of ConV-->SVM is 99.51%, <br> while ConV-->FC is 99.38% | **ConV (6L) --> FC (1Lx512, 1Lx256)**        |\n",
    "|** Test set accuracy of my architecture** |        | <font color=red>**99.51%** </font> [9 epochs + SVM]       | <font color=red> **87.53%** </font> [25 epochs]        |\n",
    "| **Effects of using adaptive learning rate** |        | • No improvement observed compared to ConV --> FC model (93.34%) <br> • Converge 3 epochs later      | • Slightly lower compared to unmodified model (85.2%) <br> • Converge couple epochs earlier.       |\n",
    "| **Lessons learned** |        |  • Preliminary tests on combinations of loss functions and <br> optimizers showed that a combination of adadelta <br> and catagorical crossentropy yielded the best results. <br> • The introduced exponential learning rate may need to be adjusted for better performance. | • Deep layers of convolutional layers are <br> required to achieve high accuracy. <br> • Higher number of epoch will likely lead to even higher accuracy. <br> • The introduced exponential learning rate may need to be adjusted for better performance.|\n",
    "\n",
    "\n",
    "*ConV = Convolutional layers, FC = Fully connected layers, SVM = Supported vector machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal 1: Following example from Keras demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "from keras import backend as K\n",
    "from keras.callbacks import EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import GridSearchCV  \n",
    "import pickle\n",
    "import seaborn as sns; sns.set()\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/12\n",
      "60000/60000 [==============================] - 120s 2ms/step - loss: 0.3305 - acc: 0.8993 - val_loss: 0.0830 - val_acc: 0.9746\n",
      "Epoch 2/12\n",
      "60000/60000 [==============================] - 115s 2ms/step - loss: 0.1154 - acc: 0.9653 - val_loss: 0.0542 - val_acc: 0.9834\n",
      "Epoch 3/12\n",
      "60000/60000 [==============================] - 116s 2ms/step - loss: 0.0873 - acc: 0.9742 - val_loss: 0.0415 - val_acc: 0.9872\n",
      "Epoch 4/12\n",
      "60000/60000 [==============================] - 120s 2ms/step - loss: 0.0734 - acc: 0.9781 - val_loss: 0.0414 - val_acc: 0.9869\n",
      "Epoch 5/12\n",
      "60000/60000 [==============================] - 125s 2ms/step - loss: 0.0635 - acc: 0.9812 - val_loss: 0.0343 - val_acc: 0.9885\n",
      "Epoch 6/12\n",
      "60000/60000 [==============================] - 121s 2ms/step - loss: 0.0574 - acc: 0.9833 - val_loss: 0.0343 - val_acc: 0.9883\n",
      "Epoch 7/12\n",
      "60000/60000 [==============================] - 120s 2ms/step - loss: 0.0530 - acc: 0.9838 - val_loss: 0.0310 - val_acc: 0.9890\n",
      "Epoch 8/12\n",
      "60000/60000 [==============================] - 121s 2ms/step - loss: 0.0482 - acc: 0.9855 - val_loss: 0.0320 - val_acc: 0.9890\n",
      "Epoch 9/12\n",
      "60000/60000 [==============================] - 122s 2ms/step - loss: 0.0456 - acc: 0.9865 - val_loss: 0.0305 - val_acc: 0.9896\n",
      "Epoch 10/12\n",
      "60000/60000 [==============================] - 121s 2ms/step - loss: 0.0422 - acc: 0.9874 - val_loss: 0.0286 - val_acc: 0.9901\n",
      "Epoch 11/12\n",
      "60000/60000 [==============================] - 122s 2ms/step - loss: 0.0398 - acc: 0.9879 - val_loss: 0.0325 - val_acc: 0.9891\n",
      "Epoch 12/12\n",
      "60000/60000 [==============================] - 122s 2ms/step - loss: 0.0389 - acc: 0.9880 - val_loss: 0.0326 - val_acc: 0.9889\n",
      "Test loss: 0.0325622016232\n",
      "Test accuracy: 0.9889\n"
     ]
    }
   ],
   "source": [
    "'''Trains a simple convnet on the MNIST dataset.\n",
    "Gets to 99.25% test accuracy after 12 epochs\n",
    "(there is still a lot of margin for parameter tuning).\n",
    "16 seconds per epoch on a GRID K520 GPU.\n",
    "'''\n",
    "\n",
    "#fix randomization\n",
    "np.random.seed(9)\n",
    "\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 12\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "# the data, shuffled and split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal 2: Improving the accuracy beyond the example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Investigating the loss functions and optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_1_lossFunc_optimizer(input_shape, num_classes, loss_func, optimizer):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(loss=loss_func,\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beginning model categorical_crossentropy|+|Adam.\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      " - 124s - loss: 0.2398 - acc: 0.9272 - val_loss: 0.0540 - val_acc: 0.9830\n",
      "Epoch 2/50\n",
      " - 122s - loss: 0.0832 - acc: 0.9757 - val_loss: 0.0394 - val_acc: 0.9863\n",
      "Epoch 3/50\n",
      " - 125s - loss: 0.0626 - acc: 0.9811 - val_loss: 0.0311 - val_acc: 0.9898\n",
      "Epoch 4/50\n",
      " - 127s - loss: 0.0524 - acc: 0.9847 - val_loss: 0.0290 - val_acc: 0.9905\n",
      "Epoch 5/50\n",
      " - 128s - loss: 0.0433 - acc: 0.9871 - val_loss: 0.0290 - val_acc: 0.9908\n",
      "Epoch 6/50\n",
      " - 129s - loss: 0.0371 - acc: 0.9883 - val_loss: 0.0305 - val_acc: 0.9902\n",
      "Epoch 7/50\n",
      " - 130s - loss: 0.0356 - acc: 0.9889 - val_loss: 0.0301 - val_acc: 0.9911\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_243 (Conv2D)          (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_244 (Conv2D)          (None, 24, 24, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_122 (MaxPoolin (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_243 (Dropout)        (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_122 (Flatten)        (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense_243 (Dense)            (None, 128)               1179776   \n",
      "_________________________________________________________________\n",
      "dropout_244 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_244 (Dense)            (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 1,199,882\n",
      "Trainable params: 1,199,882\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "model categorical_crossentropy|+|Adam ran successfully.\n",
      "beginning model categorical_hinge|+|Adadelta.\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      " - 133s - loss: 0.3263 - acc: 0.8386 - val_loss: 0.0742 - val_acc: 0.9659\n",
      "Epoch 2/50\n",
      " - 131s - loss: 0.1011 - acc: 0.9531 - val_loss: 0.0534 - val_acc: 0.9743\n",
      "Epoch 3/50\n",
      " - 131s - loss: 0.0786 - acc: 0.9633 - val_loss: 0.0456 - val_acc: 0.9779\n",
      "Epoch 4/50\n",
      " - 131s - loss: 0.0673 - acc: 0.9685 - val_loss: 0.0393 - val_acc: 0.9818\n",
      "Epoch 5/50\n",
      " - 131s - loss: 0.0615 - acc: 0.9705 - val_loss: 0.0354 - val_acc: 0.9828\n",
      "Epoch 6/50\n",
      " - 131s - loss: 0.0552 - acc: 0.9740 - val_loss: 0.0344 - val_acc: 0.9834\n",
      "Epoch 7/50\n",
      " - 131s - loss: 0.0507 - acc: 0.9761 - val_loss: 0.0333 - val_acc: 0.9839\n",
      "Epoch 8/50\n",
      " - 131s - loss: 0.0472 - acc: 0.9778 - val_loss: 0.0302 - val_acc: 0.9853\n",
      "Epoch 9/50\n",
      " - 132s - loss: 0.0444 - acc: 0.9792 - val_loss: 0.0320 - val_acc: 0.9842\n",
      "Epoch 10/50\n",
      " - 131s - loss: 0.0410 - acc: 0.9807 - val_loss: 0.0294 - val_acc: 0.9859\n",
      "Epoch 11/50\n",
      " - 131s - loss: 0.0395 - acc: 0.9814 - val_loss: 0.0266 - val_acc: 0.9872\n",
      "Epoch 12/50\n",
      " - 131s - loss: 0.0373 - acc: 0.9824 - val_loss: 0.0272 - val_acc: 0.9867\n",
      "Epoch 13/50\n",
      " - 131s - loss: 0.0359 - acc: 0.9832 - val_loss: 0.0271 - val_acc: 0.9871\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_245 (Conv2D)          (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_246 (Conv2D)          (None, 24, 24, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_123 (MaxPoolin (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_245 (Dropout)        (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_123 (Flatten)        (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense_245 (Dense)            (None, 128)               1179776   \n",
      "_________________________________________________________________\n",
      "dropout_246 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_246 (Dense)            (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 1,199,882\n",
      "Trainable params: 1,199,882\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "model categorical_hinge|+|Adadelta ran successfully.\n",
      "beginning model mean_squared_logarithmic_error|+|RMSprop.\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      " - 130s - loss: 0.0050 - acc: 0.9310 - val_loss: 0.0014 - val_acc: 0.9804\n",
      "Epoch 2/50\n",
      " - 128s - loss: 0.0020 - acc: 0.9731 - val_loss: 9.8409e-04 - val_acc: 0.9859\n",
      "Epoch 3/50\n",
      " - 128s - loss: 0.0016 - acc: 0.9800 - val_loss: 9.4037e-04 - val_acc: 0.9870\n",
      "Epoch 4/50\n",
      " - 128s - loss: 0.0013 - acc: 0.9826 - val_loss: 0.0010 - val_acc: 0.9864\n",
      "Epoch 5/50\n",
      " - 128s - loss: 0.0012 - acc: 0.9847 - val_loss: 7.8800e-04 - val_acc: 0.9901\n",
      "Epoch 6/50\n",
      " - 129s - loss: 0.0011 - acc: 0.9861 - val_loss: 8.5453e-04 - val_acc: 0.9882\n",
      "Epoch 7/50\n",
      " - 141s - loss: 9.9376e-04 - acc: 0.9872 - val_loss: 8.7228e-04 - val_acc: 0.9885\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_247 (Conv2D)          (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_248 (Conv2D)          (None, 24, 24, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_124 (MaxPoolin (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_247 (Dropout)        (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_124 (Flatten)        (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense_247 (Dense)            (None, 128)               1179776   \n",
      "_________________________________________________________________\n",
      "dropout_248 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_248 (Dense)            (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 1,199,882\n",
      "Trainable params: 1,199,882\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "model mean_squared_logarithmic_error|+|RMSprop ran successfully.\n",
      "beginning model categorical_crossentropy|+|SGD.\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      " - 136s - loss: 1.0065 - acc: 0.6816 - val_loss: 0.3197 - val_acc: 0.9059\n",
      "Epoch 2/50\n",
      " - 127s - loss: 0.4337 - acc: 0.8663 - val_loss: 0.2308 - val_acc: 0.9335\n",
      "Epoch 3/50\n",
      " - 126s - loss: 0.3638 - acc: 0.8900 - val_loss: 0.1968 - val_acc: 0.9428\n",
      "Epoch 4/50\n",
      " - 137s - loss: 0.3188 - acc: 0.9040 - val_loss: 0.1681 - val_acc: 0.9503\n",
      "Epoch 5/50\n",
      " - 137s - loss: 0.2893 - acc: 0.9128 - val_loss: 0.1543 - val_acc: 0.9536\n",
      "Epoch 6/50\n",
      " - 133s - loss: 0.2674 - acc: 0.9198 - val_loss: 0.1423 - val_acc: 0.9570\n",
      "Epoch 7/50\n",
      " - 126s - loss: 0.2502 - acc: 0.9253 - val_loss: 0.1320 - val_acc: 0.9599\n",
      "Epoch 8/50\n",
      " - 127s - loss: 0.2306 - acc: 0.9319 - val_loss: 0.1249 - val_acc: 0.9630\n",
      "Epoch 9/50\n",
      " - 144s - loss: 0.2202 - acc: 0.9348 - val_loss: 0.1160 - val_acc: 0.9658\n",
      "Epoch 10/50\n",
      " - 135s - loss: 0.2114 - acc: 0.9367 - val_loss: 0.1105 - val_acc: 0.9669\n",
      "Epoch 11/50\n",
      " - 136s - loss: 0.1952 - acc: 0.9413 - val_loss: 0.1030 - val_acc: 0.9689\n",
      "Epoch 12/50\n",
      " - 144s - loss: 0.1842 - acc: 0.9438 - val_loss: 0.0958 - val_acc: 0.9714\n",
      "Epoch 13/50\n",
      " - 134s - loss: 0.1753 - acc: 0.9484 - val_loss: 0.0907 - val_acc: 0.9721\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/50\n",
      " - 138s - loss: 0.1681 - acc: 0.9509 - val_loss: 0.0857 - val_acc: 0.9737\n",
      "Epoch 15/50\n",
      " - 138s - loss: 0.1564 - acc: 0.9543 - val_loss: 0.0838 - val_acc: 0.9740\n",
      "Epoch 16/50\n",
      " - 138s - loss: 0.1502 - acc: 0.9552 - val_loss: 0.0768 - val_acc: 0.9768\n",
      "Epoch 17/50\n",
      " - 126s - loss: 0.1438 - acc: 0.9577 - val_loss: 0.0732 - val_acc: 0.9783\n",
      "Epoch 18/50\n",
      " - 127s - loss: 0.1356 - acc: 0.9594 - val_loss: 0.0697 - val_acc: 0.9796\n",
      "Epoch 19/50\n",
      " - 126s - loss: 0.1271 - acc: 0.9620 - val_loss: 0.0665 - val_acc: 0.9793\n",
      "Epoch 20/50\n",
      " - 126s - loss: 0.1259 - acc: 0.9622 - val_loss: 0.0634 - val_acc: 0.9795\n",
      "Epoch 21/50\n",
      " - 128s - loss: 0.1203 - acc: 0.9634 - val_loss: 0.0617 - val_acc: 0.9808\n",
      "Epoch 22/50\n",
      " - 134s - loss: 0.1176 - acc: 0.9652 - val_loss: 0.0592 - val_acc: 0.9822\n",
      "Epoch 23/50\n",
      " - 126s - loss: 0.1138 - acc: 0.9664 - val_loss: 0.0576 - val_acc: 0.9819\n",
      "Epoch 24/50\n",
      " - 127s - loss: 0.1110 - acc: 0.9669 - val_loss: 0.0564 - val_acc: 0.9825\n",
      "Epoch 25/50\n",
      " - 128s - loss: 0.1044 - acc: 0.9689 - val_loss: 0.0548 - val_acc: 0.9826\n",
      "Epoch 26/50\n",
      " - 126s - loss: 0.1035 - acc: 0.9693 - val_loss: 0.0530 - val_acc: 0.9833\n",
      "Epoch 27/50\n",
      " - 126s - loss: 0.0989 - acc: 0.9695 - val_loss: 0.0526 - val_acc: 0.9836\n",
      "Epoch 28/50\n",
      " - 126s - loss: 0.0979 - acc: 0.9706 - val_loss: 0.0507 - val_acc: 0.9837\n",
      "Epoch 29/50\n",
      " - 125s - loss: 0.0941 - acc: 0.9721 - val_loss: 0.0502 - val_acc: 0.9840\n",
      "Epoch 30/50\n",
      " - 126s - loss: 0.0937 - acc: 0.9719 - val_loss: 0.0482 - val_acc: 0.9851\n",
      "Epoch 31/50\n",
      " - 126s - loss: 0.0905 - acc: 0.9724 - val_loss: 0.0468 - val_acc: 0.9849\n",
      "Epoch 32/50\n",
      " - 126s - loss: 0.0881 - acc: 0.9730 - val_loss: 0.0461 - val_acc: 0.9852\n",
      "Epoch 33/50\n",
      " - 126s - loss: 0.0855 - acc: 0.9735 - val_loss: 0.0475 - val_acc: 0.9856\n",
      "Epoch 34/50\n",
      " - 126s - loss: 0.0841 - acc: 0.9743 - val_loss: 0.0455 - val_acc: 0.9856\n",
      "Epoch 35/50\n",
      " - 126s - loss: 0.0807 - acc: 0.9751 - val_loss: 0.0449 - val_acc: 0.9863\n",
      "Epoch 36/50\n",
      " - 126s - loss: 0.0821 - acc: 0.9749 - val_loss: 0.0438 - val_acc: 0.9861\n",
      "Epoch 37/50\n",
      " - 126s - loss: 0.0791 - acc: 0.9754 - val_loss: 0.0432 - val_acc: 0.9865\n",
      "Epoch 38/50\n",
      " - 125s - loss: 0.0774 - acc: 0.9766 - val_loss: 0.0431 - val_acc: 0.9869\n",
      "Epoch 39/50\n",
      " - 126s - loss: 0.0762 - acc: 0.9759 - val_loss: 0.0424 - val_acc: 0.9868\n",
      "Epoch 40/50\n",
      " - 125s - loss: 0.0727 - acc: 0.9775 - val_loss: 0.0419 - val_acc: 0.9863\n",
      "Epoch 41/50\n",
      " - 125s - loss: 0.0731 - acc: 0.9781 - val_loss: 0.0413 - val_acc: 0.9873\n",
      "Epoch 42/50\n",
      " - 126s - loss: 0.0705 - acc: 0.9789 - val_loss: 0.0393 - val_acc: 0.9869\n",
      "Epoch 43/50\n",
      " - 126s - loss: 0.0701 - acc: 0.9778 - val_loss: 0.0404 - val_acc: 0.9868\n",
      "Epoch 44/50\n",
      " - 126s - loss: 0.0711 - acc: 0.9779 - val_loss: 0.0375 - val_acc: 0.9884\n",
      "Epoch 45/50\n",
      " - 126s - loss: 0.0675 - acc: 0.9790 - val_loss: 0.0389 - val_acc: 0.9875\n",
      "Epoch 46/50\n",
      " - 126s - loss: 0.0676 - acc: 0.9796 - val_loss: 0.0377 - val_acc: 0.9875\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_249 (Conv2D)          (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_250 (Conv2D)          (None, 24, 24, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_125 (MaxPoolin (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_249 (Dropout)        (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_125 (Flatten)        (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense_249 (Dense)            (None, 128)               1179776   \n",
      "_________________________________________________________________\n",
      "dropout_250 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_250 (Dense)            (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 1,199,882\n",
      "Trainable params: 1,199,882\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "model categorical_crossentropy|+|SGD ran successfully.\n",
      "beginning model categorical_hinge|+|Adam.\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      " - 132s - loss: 0.2193 - acc: 0.8956 - val_loss: 0.0597 - val_acc: 0.9712\n",
      "Epoch 2/50\n",
      " - 131s - loss: 0.0853 - acc: 0.9592 - val_loss: 0.0408 - val_acc: 0.9800\n",
      "Epoch 3/50\n",
      " - 130s - loss: 0.0679 - acc: 0.9675 - val_loss: 0.0399 - val_acc: 0.9804\n",
      "Epoch 4/50\n",
      " - 129s - loss: 0.0611 - acc: 0.9705 - val_loss: 0.0321 - val_acc: 0.9844\n",
      "Epoch 5/50\n",
      " - 129s - loss: 0.0533 - acc: 0.9742 - val_loss: 0.0289 - val_acc: 0.9860\n",
      "Epoch 6/50\n",
      " - 129s - loss: 0.0479 - acc: 0.9768 - val_loss: 0.0282 - val_acc: 0.9867\n",
      "Epoch 7/50\n",
      " - 129s - loss: 0.0444 - acc: 0.9782 - val_loss: 0.0256 - val_acc: 0.9879\n",
      "Epoch 8/50\n",
      " - 129s - loss: 0.0428 - acc: 0.9791 - val_loss: 0.0252 - val_acc: 0.9881\n",
      "Epoch 9/50\n",
      " - 129s - loss: 0.0414 - acc: 0.9797 - val_loss: 0.0243 - val_acc: 0.9876\n",
      "Epoch 10/50\n",
      " - 129s - loss: 0.0393 - acc: 0.9806 - val_loss: 0.0231 - val_acc: 0.9888\n",
      "Epoch 11/50\n",
      " - 129s - loss: 0.0399 - acc: 0.9803 - val_loss: 0.0232 - val_acc: 0.9884\n",
      "Epoch 12/50\n",
      " - 129s - loss: 0.0352 - acc: 0.9827 - val_loss: 0.0233 - val_acc: 0.9884\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_251 (Conv2D)          (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_252 (Conv2D)          (None, 24, 24, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_126 (MaxPoolin (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_251 (Dropout)        (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_126 (Flatten)        (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense_251 (Dense)            (None, 128)               1179776   \n",
      "_________________________________________________________________\n",
      "dropout_252 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_252 (Dense)            (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 1,199,882\n",
      "Trainable params: 1,199,882\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "model categorical_hinge|+|Adam ran successfully.\n",
      "beginning model mean_squared_logarithmic_error|+|Adadelta.\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      " - 133s - loss: 0.0180 - acc: 0.7343 - val_loss: 0.0059 - val_acc: 0.9217\n",
      "Epoch 2/50\n",
      " - 131s - loss: 0.0082 - acc: 0.8899 - val_loss: 0.0044 - val_acc: 0.9431\n",
      "Epoch 3/50\n",
      " - 132s - loss: 0.0066 - acc: 0.9111 - val_loss: 0.0037 - val_acc: 0.9500\n",
      "Epoch 4/50\n",
      " - 132s - loss: 0.0057 - acc: 0.9245 - val_loss: 0.0032 - val_acc: 0.9572\n",
      "Epoch 5/50\n",
      " - 131s - loss: 0.0048 - acc: 0.9368 - val_loss: 0.0028 - val_acc: 0.9639\n",
      "Epoch 6/50\n",
      " - 131s - loss: 0.0043 - acc: 0.9443 - val_loss: 0.0023 - val_acc: 0.9688\n",
      "Epoch 7/50\n",
      " - 131s - loss: 0.0037 - acc: 0.9514 - val_loss: 0.0021 - val_acc: 0.9713\n",
      "Epoch 8/50\n",
      " - 131s - loss: 0.0033 - acc: 0.9572 - val_loss: 0.0020 - val_acc: 0.9724\n",
      "Epoch 9/50\n",
      " - 132s - loss: 0.0030 - acc: 0.9617 - val_loss: 0.0017 - val_acc: 0.9760\n",
      "Epoch 10/50\n",
      " - 131s - loss: 0.0028 - acc: 0.9640 - val_loss: 0.0016 - val_acc: 0.9780\n",
      "Epoch 11/50\n",
      " - 131s - loss: 0.0026 - acc: 0.9660 - val_loss: 0.0015 - val_acc: 0.9788\n",
      "Epoch 12/50\n",
      " - 131s - loss: 0.0025 - acc: 0.9684 - val_loss: 0.0014 - val_acc: 0.9808\n",
      "Epoch 13/50\n",
      " - 132s - loss: 0.0023 - acc: 0.9705 - val_loss: 0.0014 - val_acc: 0.9801\n",
      "Epoch 14/50\n",
      " - 132s - loss: 0.0022 - acc: 0.9715 - val_loss: 0.0013 - val_acc: 0.9823\n",
      "Epoch 15/50\n",
      " - 132s - loss: 0.0021 - acc: 0.9734 - val_loss: 0.0013 - val_acc: 0.9828\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/50\n",
      " - 132s - loss: 0.0020 - acc: 0.9738 - val_loss: 0.0012 - val_acc: 0.9835\n",
      "Epoch 17/50\n",
      " - 132s - loss: 0.0019 - acc: 0.9755 - val_loss: 0.0012 - val_acc: 0.9835\n",
      "Epoch 18/50\n",
      " - 132s - loss: 0.0018 - acc: 0.9770 - val_loss: 0.0011 - val_acc: 0.9842\n",
      "Epoch 19/50\n",
      " - 131s - loss: 0.0017 - acc: 0.9782 - val_loss: 0.0011 - val_acc: 0.9849\n",
      "Epoch 20/50\n",
      " - 132s - loss: 0.0017 - acc: 0.9785 - val_loss: 0.0010 - val_acc: 0.9853\n",
      "Epoch 21/50\n",
      " - 131s - loss: 0.0016 - acc: 0.9792 - val_loss: 0.0010 - val_acc: 0.9851\n",
      "Epoch 22/50\n",
      " - 131s - loss: 0.0016 - acc: 0.9788 - val_loss: 9.9404e-04 - val_acc: 0.9852\n",
      "Epoch 23/50\n",
      " - 131s - loss: 0.0015 - acc: 0.9802 - val_loss: 9.7835e-04 - val_acc: 0.9862\n",
      "Epoch 24/50\n",
      " - 131s - loss: 0.0015 - acc: 0.9809 - val_loss: 9.8443e-04 - val_acc: 0.9862\n",
      "Epoch 25/50\n",
      " - 131s - loss: 0.0015 - acc: 0.9812 - val_loss: 9.7105e-04 - val_acc: 0.9863\n",
      "Epoch 26/50\n",
      " - 131s - loss: 0.0014 - acc: 0.9813 - val_loss: 9.4598e-04 - val_acc: 0.9870\n",
      "Epoch 27/50\n",
      " - 131s - loss: 0.0014 - acc: 0.9819 - val_loss: 9.2289e-04 - val_acc: 0.9868\n",
      "Epoch 28/50\n",
      " - 131s - loss: 0.0014 - acc: 0.9824 - val_loss: 9.1920e-04 - val_acc: 0.9868\n",
      "Epoch 29/50\n",
      " - 131s - loss: 0.0013 - acc: 0.9828 - val_loss: 9.1098e-04 - val_acc: 0.9863\n",
      "Epoch 30/50\n",
      " - 131s - loss: 0.0013 - acc: 0.9830 - val_loss: 9.0571e-04 - val_acc: 0.9870\n",
      "Epoch 31/50\n",
      " - 131s - loss: 0.0013 - acc: 0.9835 - val_loss: 8.5956e-04 - val_acc: 0.9875\n",
      "Epoch 32/50\n",
      " - 130s - loss: 0.0013 - acc: 0.9838 - val_loss: 8.4817e-04 - val_acc: 0.9873\n",
      "Epoch 33/50\n",
      " - 131s - loss: 0.0012 - acc: 0.9849 - val_loss: 8.6652e-04 - val_acc: 0.9874\n",
      "Epoch 34/50\n",
      " - 131s - loss: 0.0012 - acc: 0.9847 - val_loss: 8.2925e-04 - val_acc: 0.9876\n",
      "Epoch 35/50\n",
      " - 131s - loss: 0.0012 - acc: 0.9847 - val_loss: 8.4938e-04 - val_acc: 0.9880\n",
      "Epoch 36/50\n",
      " - 131s - loss: 0.0012 - acc: 0.9849 - val_loss: 8.0184e-04 - val_acc: 0.9885\n",
      "Epoch 37/50\n",
      " - 131s - loss: 0.0012 - acc: 0.9852 - val_loss: 8.3149e-04 - val_acc: 0.9878\n",
      "Epoch 38/50\n",
      " - 131s - loss: 0.0011 - acc: 0.9858 - val_loss: 8.2858e-04 - val_acc: 0.9881\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_253 (Conv2D)          (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_254 (Conv2D)          (None, 24, 24, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_127 (MaxPoolin (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_253 (Dropout)        (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_127 (Flatten)        (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense_253 (Dense)            (None, 128)               1179776   \n",
      "_________________________________________________________________\n",
      "dropout_254 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_254 (Dense)            (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 1,199,882\n",
      "Trainable params: 1,199,882\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "model mean_squared_logarithmic_error|+|Adadelta ran successfully.\n",
      "beginning model categorical_crossentropy|+|RMSprop.\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      " - 130s - loss: 0.2144 - acc: 0.9349 - val_loss: 0.0539 - val_acc: 0.9831\n",
      "Epoch 2/50\n",
      " - 128s - loss: 0.0780 - acc: 0.9771 - val_loss: 0.0421 - val_acc: 0.9868\n",
      "Epoch 3/50\n",
      " - 127s - loss: 0.0598 - acc: 0.9827 - val_loss: 0.0326 - val_acc: 0.9902\n",
      "Epoch 4/50\n",
      " - 127s - loss: 0.0530 - acc: 0.9842 - val_loss: 0.0342 - val_acc: 0.9883\n",
      "Epoch 5/50\n",
      " - 127s - loss: 0.0461 - acc: 0.9865 - val_loss: 0.0328 - val_acc: 0.9892\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_255 (Conv2D)          (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_256 (Conv2D)          (None, 24, 24, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_128 (MaxPoolin (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_255 (Dropout)        (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_128 (Flatten)        (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense_255 (Dense)            (None, 128)               1179776   \n",
      "_________________________________________________________________\n",
      "dropout_256 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_256 (Dense)            (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 1,199,882\n",
      "Trainable params: 1,199,882\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "model categorical_crossentropy|+|RMSprop ran successfully.\n",
      "beginning model categorical_hinge|+|SGD.\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      " - 127s - loss: 1.0054 - acc: 0.2091 - val_loss: 0.9997 - val_acc: 0.5275\n",
      "Epoch 2/50\n",
      " - 125s - loss: 1.0014 - acc: 0.3371 - val_loss: 0.9991 - val_acc: 0.6251\n",
      "Epoch 3/50\n",
      " - 125s - loss: 1.0005 - acc: 0.4130 - val_loss: 0.9985 - val_acc: 0.6891\n",
      "Epoch 4/50\n",
      " - 125s - loss: 0.9998 - acc: 0.4755 - val_loss: 0.9975 - val_acc: 0.7335\n",
      "Epoch 5/50\n",
      " - 125s - loss: 0.9987 - acc: 0.5218 - val_loss: 0.9952 - val_acc: 0.7590\n",
      "Epoch 6/50\n",
      " - 125s - loss: 0.9962 - acc: 0.5626 - val_loss: 0.9888 - val_acc: 0.7678\n",
      "Epoch 7/50\n",
      " - 126s - loss: 0.9767 - acc: 0.5869 - val_loss: 0.8936 - val_acc: 0.7389\n",
      "Epoch 8/50\n",
      " - 125s - loss: 0.6339 - acc: 0.7220 - val_loss: 0.2763 - val_acc: 0.8852\n",
      "Epoch 9/50\n",
      " - 125s - loss: 0.3458 - acc: 0.8435 - val_loss: 0.1986 - val_acc: 0.9095\n",
      "Epoch 10/50\n",
      " - 125s - loss: 0.2818 - acc: 0.8691 - val_loss: 0.1699 - val_acc: 0.9201\n",
      "Epoch 11/50\n",
      " - 125s - loss: 0.2487 - acc: 0.8829 - val_loss: 0.1552 - val_acc: 0.9267\n",
      "Epoch 12/50\n",
      " - 125s - loss: 0.2300 - acc: 0.8912 - val_loss: 0.1386 - val_acc: 0.9344\n",
      "Epoch 13/50\n",
      " - 125s - loss: 0.2143 - acc: 0.8987 - val_loss: 0.1321 - val_acc: 0.9376\n",
      "Epoch 14/50\n",
      " - 126s - loss: 0.2023 - acc: 0.9036 - val_loss: 0.1254 - val_acc: 0.9405\n",
      "Epoch 15/50\n",
      " - 125s - loss: 0.1932 - acc: 0.9076 - val_loss: 0.1173 - val_acc: 0.9445\n",
      "Epoch 16/50\n",
      " - 125s - loss: 0.1874 - acc: 0.9108 - val_loss: 0.1123 - val_acc: 0.9452\n",
      "Epoch 17/50\n",
      " - 125s - loss: 0.1788 - acc: 0.9142 - val_loss: 0.1110 - val_acc: 0.9469\n",
      "Epoch 18/50\n",
      " - 125s - loss: 0.1731 - acc: 0.9167 - val_loss: 0.1073 - val_acc: 0.9486\n",
      "Epoch 19/50\n",
      " - 125s - loss: 0.1675 - acc: 0.9195 - val_loss: 0.1028 - val_acc: 0.9503\n",
      "Epoch 20/50\n",
      " - 125s - loss: 0.1616 - acc: 0.9227 - val_loss: 0.0971 - val_acc: 0.9528\n",
      "Epoch 21/50\n",
      " - 125s - loss: 0.1593 - acc: 0.9235 - val_loss: 0.0979 - val_acc: 0.9529\n",
      "Epoch 22/50\n",
      " - 125s - loss: 0.1554 - acc: 0.9257 - val_loss: 0.0940 - val_acc: 0.9550\n",
      "Epoch 23/50\n",
      " - 125s - loss: 0.1513 - acc: 0.9274 - val_loss: 0.0913 - val_acc: 0.9562\n",
      "Epoch 24/50\n",
      " - 125s - loss: 0.1481 - acc: 0.9286 - val_loss: 0.0893 - val_acc: 0.9578\n",
      "Epoch 25/50\n",
      " - 125s - loss: 0.1451 - acc: 0.9302 - val_loss: 0.0894 - val_acc: 0.9565\n",
      "Epoch 26/50\n",
      " - 125s - loss: 0.1429 - acc: 0.9314 - val_loss: 0.0867 - val_acc: 0.9589\n",
      "Epoch 27/50\n",
      " - 125s - loss: 0.1411 - acc: 0.9323 - val_loss: 0.0858 - val_acc: 0.9574\n",
      "Epoch 28/50\n",
      " - 125s - loss: 0.1371 - acc: 0.9339 - val_loss: 0.0825 - val_acc: 0.9598\n",
      "Epoch 29/50\n",
      " - 125s - loss: 0.1356 - acc: 0.9344 - val_loss: 0.0831 - val_acc: 0.9602\n",
      "Epoch 30/50\n",
      " - 125s - loss: 0.1336 - acc: 0.9358 - val_loss: 0.0818 - val_acc: 0.9600\n",
      "Epoch 31/50\n",
      " - 126s - loss: 0.1315 - acc: 0.9365 - val_loss: 0.0796 - val_acc: 0.9606\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/50\n",
      " - 125s - loss: 0.1295 - acc: 0.9368 - val_loss: 0.0779 - val_acc: 0.9630\n",
      "Epoch 33/50\n",
      " - 125s - loss: 0.1254 - acc: 0.9396 - val_loss: 0.0764 - val_acc: 0.9631\n",
      "Epoch 34/50\n",
      " - 124s - loss: 0.1273 - acc: 0.9384 - val_loss: 0.0750 - val_acc: 0.9640\n",
      "Epoch 35/50\n",
      " - 125s - loss: 0.1248 - acc: 0.9399 - val_loss: 0.0773 - val_acc: 0.9623\n",
      "Epoch 36/50\n",
      " - 125s - loss: 0.1211 - acc: 0.9413 - val_loss: 0.0746 - val_acc: 0.9634\n",
      "Epoch 37/50\n",
      " - 125s - loss: 0.1203 - acc: 0.9422 - val_loss: 0.0706 - val_acc: 0.9661\n",
      "Epoch 38/50\n",
      " - 125s - loss: 0.1182 - acc: 0.9430 - val_loss: 0.0714 - val_acc: 0.9652\n",
      "Epoch 39/50\n",
      " - 125s - loss: 0.1173 - acc: 0.9433 - val_loss: 0.0705 - val_acc: 0.9651\n",
      "Epoch 40/50\n",
      " - 125s - loss: 0.1147 - acc: 0.9449 - val_loss: 0.0684 - val_acc: 0.9667\n",
      "Epoch 41/50\n",
      " - 125s - loss: 0.1130 - acc: 0.9454 - val_loss: 0.0661 - val_acc: 0.9685\n",
      "Epoch 42/50\n",
      " - 125s - loss: 0.1129 - acc: 0.9452 - val_loss: 0.0668 - val_acc: 0.9681\n",
      "Epoch 43/50\n",
      " - 126s - loss: 0.1100 - acc: 0.9481 - val_loss: 0.0655 - val_acc: 0.9684\n",
      "Epoch 44/50\n",
      " - 125s - loss: 0.1096 - acc: 0.9478 - val_loss: 0.0663 - val_acc: 0.9670\n",
      "Epoch 45/50\n",
      " - 125s - loss: 0.1079 - acc: 0.9479 - val_loss: 0.0638 - val_acc: 0.9685\n",
      "Epoch 46/50\n",
      " - 125s - loss: 0.1046 - acc: 0.9499 - val_loss: 0.0639 - val_acc: 0.9693\n",
      "Epoch 47/50\n",
      " - 125s - loss: 0.1053 - acc: 0.9494 - val_loss: 0.0625 - val_acc: 0.9692\n",
      "Epoch 48/50\n",
      " - 125s - loss: 0.1024 - acc: 0.9512 - val_loss: 0.0609 - val_acc: 0.9706\n",
      "Epoch 49/50\n",
      " - 125s - loss: 0.1017 - acc: 0.9511 - val_loss: 0.0613 - val_acc: 0.9706\n",
      "Epoch 50/50\n",
      " - 125s - loss: 0.1017 - acc: 0.9512 - val_loss: 0.0604 - val_acc: 0.9709\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_257 (Conv2D)          (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_258 (Conv2D)          (None, 24, 24, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_129 (MaxPoolin (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_257 (Dropout)        (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_129 (Flatten)        (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense_257 (Dense)            (None, 128)               1179776   \n",
      "_________________________________________________________________\n",
      "dropout_258 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_258 (Dense)            (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 1,199,882\n",
      "Trainable params: 1,199,882\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "model categorical_hinge|+|SGD ran successfully.\n",
      "beginning model mean_squared_logarithmic_error|+|Adam.\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      " - 131s - loss: 0.0054 - acc: 0.9254 - val_loss: 0.0015 - val_acc: 0.9803\n",
      "Epoch 2/50\n",
      " - 129s - loss: 0.0021 - acc: 0.9725 - val_loss: 0.0010 - val_acc: 0.9865\n",
      "Epoch 3/50\n",
      " - 129s - loss: 0.0016 - acc: 0.9781 - val_loss: 0.0011 - val_acc: 0.9854\n",
      "Epoch 4/50\n",
      " - 129s - loss: 0.0014 - acc: 0.9821 - val_loss: 8.9283e-04 - val_acc: 0.9879\n",
      "Epoch 5/50\n",
      " - 129s - loss: 0.0012 - acc: 0.9841 - val_loss: 7.1398e-04 - val_acc: 0.9899\n",
      "Epoch 6/50\n",
      " - 129s - loss: 0.0011 - acc: 0.9856 - val_loss: 8.3729e-04 - val_acc: 0.9888\n",
      "Epoch 7/50\n",
      " - 129s - loss: 9.9515e-04 - acc: 0.9871 - val_loss: 6.8388e-04 - val_acc: 0.9911\n",
      "Epoch 8/50\n",
      " - 129s - loss: 9.5665e-04 - acc: 0.9877 - val_loss: 7.1530e-04 - val_acc: 0.9906\n",
      "Epoch 9/50\n",
      " - 129s - loss: 8.5809e-04 - acc: 0.9889 - val_loss: 6.6820e-04 - val_acc: 0.9914\n",
      "Epoch 10/50\n",
      " - 129s - loss: 8.4127e-04 - acc: 0.9891 - val_loss: 6.6719e-04 - val_acc: 0.9911\n",
      "Epoch 11/50\n",
      " - 129s - loss: 7.6282e-04 - acc: 0.9901 - val_loss: 6.8688e-04 - val_acc: 0.9908\n",
      "Epoch 12/50\n",
      " - 128s - loss: 7.0358e-04 - acc: 0.9910 - val_loss: 7.4172e-04 - val_acc: 0.9905\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_259 (Conv2D)          (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_260 (Conv2D)          (None, 24, 24, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_130 (MaxPoolin (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_259 (Dropout)        (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_130 (Flatten)        (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense_259 (Dense)            (None, 128)               1179776   \n",
      "_________________________________________________________________\n",
      "dropout_260 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_260 (Dense)            (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 1,199,882\n",
      "Trainable params: 1,199,882\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "model mean_squared_logarithmic_error|+|Adam ran successfully.\n",
      "beginning model categorical_crossentropy|+|Adadelta.\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      " - 133s - loss: 0.3298 - acc: 0.8994 - val_loss: 0.0806 - val_acc: 0.9737\n",
      "Epoch 2/50\n",
      " - 132s - loss: 0.1160 - acc: 0.9661 - val_loss: 0.0546 - val_acc: 0.9833\n",
      "Epoch 3/50\n",
      " - 131s - loss: 0.0840 - acc: 0.9738 - val_loss: 0.0464 - val_acc: 0.9846\n",
      "Epoch 4/50\n",
      " - 131s - loss: 0.0704 - acc: 0.9795 - val_loss: 0.0382 - val_acc: 0.9862\n",
      "Epoch 5/50\n",
      " - 131s - loss: 0.0615 - acc: 0.9819 - val_loss: 0.0366 - val_acc: 0.9877\n",
      "Epoch 6/50\n",
      " - 131s - loss: 0.0565 - acc: 0.9829 - val_loss: 0.0336 - val_acc: 0.9892\n",
      "Epoch 7/50\n",
      " - 131s - loss: 0.0497 - acc: 0.9854 - val_loss: 0.0333 - val_acc: 0.9893\n",
      "Epoch 8/50\n",
      " - 131s - loss: 0.0466 - acc: 0.9864 - val_loss: 0.0311 - val_acc: 0.9893\n",
      "Epoch 9/50\n",
      " - 131s - loss: 0.0455 - acc: 0.9868 - val_loss: 0.0292 - val_acc: 0.9907\n",
      "Epoch 10/50\n",
      " - 131s - loss: 0.0405 - acc: 0.9873 - val_loss: 0.0300 - val_acc: 0.9903\n",
      "Epoch 11/50\n",
      " - 131s - loss: 0.0396 - acc: 0.9878 - val_loss: 0.0274 - val_acc: 0.9909\n",
      "Epoch 12/50\n",
      " - 131s - loss: 0.0375 - acc: 0.9887 - val_loss: 0.0275 - val_acc: 0.9917\n",
      "Epoch 13/50\n",
      " - 131s - loss: 0.0356 - acc: 0.9888 - val_loss: 0.0283 - val_acc: 0.9912\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_261 (Conv2D)          (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_262 (Conv2D)          (None, 24, 24, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_131 (MaxPoolin (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_261 (Dropout)        (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_131 (Flatten)        (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense_261 (Dense)            (None, 128)               1179776   \n",
      "_________________________________________________________________\n",
      "dropout_262 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_262 (Dense)            (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 1,199,882\n",
      "Trainable params: 1,199,882\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "model categorical_crossentropy|+|Adadelta ran successfully.\n",
      "beginning model categorical_hinge|+|RMSprop.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      " - 130s - loss: 0.2400 - acc: 0.8846 - val_loss: 0.0593 - val_acc: 0.9712\n",
      "Epoch 2/50\n",
      " - 128s - loss: 0.0959 - acc: 0.9540 - val_loss: 0.0432 - val_acc: 0.9788\n",
      "Epoch 3/50\n",
      " - 128s - loss: 0.0788 - acc: 0.9619 - val_loss: 0.0406 - val_acc: 0.9796\n",
      "Epoch 4/50\n",
      " - 127s - loss: 0.0675 - acc: 0.9672 - val_loss: 0.0391 - val_acc: 0.9804\n",
      "Epoch 5/50\n",
      " - 128s - loss: 0.0606 - acc: 0.9704 - val_loss: 0.0319 - val_acc: 0.9840\n",
      "Epoch 6/50\n",
      " - 128s - loss: 0.0565 - acc: 0.9723 - val_loss: 0.0292 - val_acc: 0.9857\n",
      "Epoch 7/50\n",
      " - 127s - loss: 0.0526 - acc: 0.9741 - val_loss: 0.0340 - val_acc: 0.9834\n",
      "Epoch 8/50\n",
      " - 127s - loss: 0.0482 - acc: 0.9764 - val_loss: 0.0284 - val_acc: 0.9861\n",
      "Epoch 9/50\n",
      " - 128s - loss: 0.0467 - acc: 0.9770 - val_loss: 0.0292 - val_acc: 0.9852\n",
      "Epoch 10/50\n",
      " - 128s - loss: 0.0464 - acc: 0.9770 - val_loss: 0.0290 - val_acc: 0.9854\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_263 (Conv2D)          (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_264 (Conv2D)          (None, 24, 24, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_132 (MaxPoolin (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_263 (Dropout)        (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_132 (Flatten)        (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense_263 (Dense)            (None, 128)               1179776   \n",
      "_________________________________________________________________\n",
      "dropout_264 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_264 (Dense)            (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 1,199,882\n",
      "Trainable params: 1,199,882\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "model categorical_hinge|+|RMSprop ran successfully.\n",
      "beginning model mean_squared_logarithmic_error|+|SGD.\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      " - 127s - loss: 0.0440 - acc: 0.1013 - val_loss: 0.0438 - val_acc: 0.1432\n",
      "Epoch 2/50\n",
      " - 126s - loss: 0.0438 - acc: 0.1259 - val_loss: 0.0437 - val_acc: 0.1988\n",
      "Epoch 3/50\n",
      " - 126s - loss: 0.0437 - acc: 0.1483 - val_loss: 0.0435 - val_acc: 0.2519\n",
      "Epoch 4/50\n",
      " - 126s - loss: 0.0436 - acc: 0.1767 - val_loss: 0.0434 - val_acc: 0.3051\n",
      "Epoch 5/50\n",
      " - 126s - loss: 0.0434 - acc: 0.2037 - val_loss: 0.0433 - val_acc: 0.3496\n",
      "Epoch 6/50\n",
      " - 126s - loss: 0.0433 - acc: 0.2299 - val_loss: 0.0431 - val_acc: 0.3858\n",
      "Epoch 7/50\n",
      " - 126s - loss: 0.0431 - acc: 0.2557 - val_loss: 0.0429 - val_acc: 0.4122\n",
      "Epoch 8/50\n",
      " - 125s - loss: 0.0429 - acc: 0.2792 - val_loss: 0.0426 - val_acc: 0.4356\n",
      "Epoch 9/50\n",
      " - 126s - loss: 0.0427 - acc: 0.3020 - val_loss: 0.0423 - val_acc: 0.4586\n",
      "Epoch 10/50\n",
      " - 126s - loss: 0.0424 - acc: 0.3264 - val_loss: 0.0420 - val_acc: 0.4898\n",
      "Epoch 11/50\n",
      " - 126s - loss: 0.0420 - acc: 0.3510 - val_loss: 0.0415 - val_acc: 0.5180\n",
      "Epoch 12/50\n",
      " - 125s - loss: 0.0415 - acc: 0.3749 - val_loss: 0.0408 - val_acc: 0.5454\n",
      "Epoch 13/50\n",
      " - 126s - loss: 0.0408 - acc: 0.3999 - val_loss: 0.0398 - val_acc: 0.5689\n",
      "Epoch 14/50\n",
      " - 126s - loss: 0.0399 - acc: 0.4316 - val_loss: 0.0385 - val_acc: 0.5974\n",
      "Epoch 15/50\n",
      " - 126s - loss: 0.0386 - acc: 0.4633 - val_loss: 0.0367 - val_acc: 0.6255\n",
      "Epoch 16/50\n",
      " - 126s - loss: 0.0368 - acc: 0.5002 - val_loss: 0.0341 - val_acc: 0.6546\n",
      "Epoch 17/50\n",
      " - 127s - loss: 0.0346 - acc: 0.5364 - val_loss: 0.0310 - val_acc: 0.6873\n",
      "Epoch 18/50\n",
      " - 126s - loss: 0.0321 - acc: 0.5687 - val_loss: 0.0275 - val_acc: 0.7178\n",
      "Epoch 19/50\n",
      " - 127s - loss: 0.0295 - acc: 0.5981 - val_loss: 0.0242 - val_acc: 0.7493\n",
      "Epoch 20/50\n",
      " - 126s - loss: 0.0271 - acc: 0.6295 - val_loss: 0.0213 - val_acc: 0.7725\n",
      "Epoch 21/50\n",
      " - 126s - loss: 0.0251 - acc: 0.6540 - val_loss: 0.0189 - val_acc: 0.7963\n",
      "Epoch 22/50\n",
      " - 126s - loss: 0.0233 - acc: 0.6768 - val_loss: 0.0170 - val_acc: 0.8172\n",
      "Epoch 23/50\n",
      " - 126s - loss: 0.0216 - acc: 0.7043 - val_loss: 0.0154 - val_acc: 0.8342\n",
      "Epoch 24/50\n",
      " - 125s - loss: 0.0205 - acc: 0.7194 - val_loss: 0.0141 - val_acc: 0.8452\n",
      "Epoch 25/50\n",
      " - 125s - loss: 0.0192 - acc: 0.7366 - val_loss: 0.0130 - val_acc: 0.8568\n",
      "Epoch 26/50\n",
      " - 126s - loss: 0.0184 - acc: 0.7481 - val_loss: 0.0122 - val_acc: 0.8617\n",
      "Epoch 27/50\n",
      " - 126s - loss: 0.0174 - acc: 0.7633 - val_loss: 0.0115 - val_acc: 0.8668\n",
      "Epoch 28/50\n",
      " - 126s - loss: 0.0169 - acc: 0.7707 - val_loss: 0.0109 - val_acc: 0.8714\n",
      "Epoch 29/50\n",
      " - 126s - loss: 0.0161 - acc: 0.7817 - val_loss: 0.0104 - val_acc: 0.8752\n",
      "Epoch 30/50\n",
      " - 126s - loss: 0.0157 - acc: 0.7865 - val_loss: 0.0100 - val_acc: 0.8784\n",
      "Epoch 31/50\n",
      " - 126s - loss: 0.0152 - acc: 0.7936 - val_loss: 0.0096 - val_acc: 0.8820\n",
      "Epoch 32/50\n",
      " - 125s - loss: 0.0148 - acc: 0.7992 - val_loss: 0.0093 - val_acc: 0.8850\n",
      "Epoch 33/50\n",
      " - 126s - loss: 0.0143 - acc: 0.8063 - val_loss: 0.0090 - val_acc: 0.8890\n",
      "Epoch 34/50\n",
      " - 126s - loss: 0.0140 - acc: 0.8107 - val_loss: 0.0087 - val_acc: 0.8903\n",
      "Epoch 35/50\n",
      " - 128s - loss: 0.0137 - acc: 0.8135 - val_loss: 0.0085 - val_acc: 0.8929\n",
      "Epoch 36/50\n",
      " - 127s - loss: 0.0134 - acc: 0.8178 - val_loss: 0.0083 - val_acc: 0.8942\n",
      "Epoch 37/50\n",
      " - 123s - loss: 0.0132 - acc: 0.8220 - val_loss: 0.0082 - val_acc: 0.8982\n",
      "Epoch 38/50\n",
      " - 123s - loss: 0.0130 - acc: 0.8236 - val_loss: 0.0080 - val_acc: 0.8998\n",
      "Epoch 39/50\n",
      " - 119s - loss: 0.0127 - acc: 0.8283 - val_loss: 0.0078 - val_acc: 0.9009\n",
      "Epoch 40/50\n",
      " - 113s - loss: 0.0126 - acc: 0.8304 - val_loss: 0.0077 - val_acc: 0.9021\n",
      "Epoch 41/50\n",
      " - 112s - loss: 0.0124 - acc: 0.8318 - val_loss: 0.0076 - val_acc: 0.9037\n",
      "Epoch 42/50\n",
      " - 117s - loss: 0.0121 - acc: 0.8352 - val_loss: 0.0074 - val_acc: 0.9046\n",
      "Epoch 43/50\n",
      " - 112s - loss: 0.0119 - acc: 0.8394 - val_loss: 0.0073 - val_acc: 0.9062\n",
      "Epoch 44/50\n",
      " - 112s - loss: 0.0118 - acc: 0.8406 - val_loss: 0.0072 - val_acc: 0.9070\n",
      "Epoch 45/50\n",
      " - 111s - loss: 0.0115 - acc: 0.8446 - val_loss: 0.0071 - val_acc: 0.9091\n",
      "Epoch 46/50\n",
      " - 111s - loss: 0.0115 - acc: 0.8440 - val_loss: 0.0070 - val_acc: 0.9101\n",
      "Epoch 47/50\n",
      " - 112s - loss: 0.0113 - acc: 0.8467 - val_loss: 0.0069 - val_acc: 0.9108\n",
      "Epoch 48/50\n",
      " - 113s - loss: 0.0113 - acc: 0.8465 - val_loss: 0.0069 - val_acc: 0.9113\n",
      "Epoch 49/50\n",
      " - 114s - loss: 0.0111 - acc: 0.8516 - val_loss: 0.0067 - val_acc: 0.9121\n",
      "Epoch 50/50\n",
      " - 114s - loss: 0.0110 - acc: 0.8507 - val_loss: 0.0067 - val_acc: 0.9131\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_265 (Conv2D)          (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_266 (Conv2D)          (None, 24, 24, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_133 (MaxPoolin (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_265 (Dropout)        (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_133 (Flatten)        (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense_265 (Dense)            (None, 128)               1179776   \n",
      "_________________________________________________________________\n",
      "dropout_266 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_266 (Dense)            (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 1,199,882\n",
      "Trainable params: 1,199,882\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "model mean_squared_logarithmic_error|+|SGD ran successfully.\n"
     ]
    }
   ],
   "source": [
    "loss_func = ['categorical_crossentropy', 'categorical_hinge', 'mean_squared_logarithmic_error']\n",
    "optimizer = ['Adam', 'Adadelta', 'RMSprop', 'SGD']\n",
    "history = {}\n",
    "keys = []\n",
    "model_list = {}\n",
    "for i in range(12):\n",
    "    keys.append(loss_func[i%3])\n",
    "    keys[i] += '|+|'\n",
    "    keys[i] += optimizer[i%4]\n",
    "    model_list[keys[i]] = model_1_lossFunc_optimizer(input_shape, num_classes, loss_func[i%3], optimizer[i%4])\n",
    "    \n",
    "for model in keys:\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=2)\n",
    "    print('beginning model {}.'.format(model))\n",
    "    history[model] = model_list[model].fit(x_train, y_train,\n",
    "        batch_size=batch_size,\n",
    "        verbose=2,\n",
    "        epochs=50,\n",
    "        validation_data=(x_test, y_test),\n",
    "        callbacks=[early_stopping])\n",
    "    score = model_list[model].evaluate(x_test, y_test, verbose=0)\n",
    "    model_list[model].summary()\n",
    "    print('model {} ran successfully.'.format(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for model in keys:\n",
    "    plt.plot(history[model].history['val_loss'], linewidth=3)\n",
    "#plt.legend(keys , loc='upper right')\n",
    "plt.title('val_loss VS epoch')\n",
    "plt.ylabel('val_loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.grid(color='gray', linestyle='--', linewidth=0.1)\n",
    "plt.show()\n",
    "\n",
    "for model in keys:\n",
    "    plt.plot(history[model].history['val_acc'], linewidth=3)\n",
    "plt.legend(keys , bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.title('val_acc VS epoch')\n",
    "plt.ylabel('val_acc')\n",
    "plt.xlabel('epoch')\n",
    "plt.grid(color='gray', linestyle='--', linewidth=0.1)\n",
    "plt.axis([0,50,0.98,1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this we can be sure that catagorical_crossentropy with Adadelta optimizer is one of the, if not the, best combinations for this dataset and model. Next step we will vary the model structure to using this loss function and optimizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the ConV-->Fully Connected Layers (FC) model with optimized loss functions and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning model model 2.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_5 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 26, 26, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 24, 24, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 12, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 12, 12, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 10, 10, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 10, 10, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 8, 8, 64)          36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 4, 4, 64)          256       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 597,738\n",
      "Trainable params: 596,330\n",
      "Non-trainable params: 1,408\n",
      "_________________________________________________________________\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      " - 263s - loss: 0.1551 - acc: 0.9526 - val_loss: 0.2483 - val_acc: 0.9278\n",
      "Epoch 2/50\n",
      " - 219s - loss: 0.0442 - acc: 0.9865 - val_loss: 0.0300 - val_acc: 0.9901\n",
      "Epoch 3/50\n",
      " - 224s - loss: 0.0276 - acc: 0.9913 - val_loss: 0.0229 - val_acc: 0.9927\n",
      "Epoch 4/50\n",
      " - 220s - loss: 0.0199 - acc: 0.9940 - val_loss: 0.0229 - val_acc: 0.9927\n",
      "Epoch 5/50\n",
      " - 233s - loss: 0.0148 - acc: 0.9951 - val_loss: 0.0222 - val_acc: 0.9941\n",
      "Epoch 6/50\n",
      " - 227s - loss: 0.0118 - acc: 0.9962 - val_loss: 0.0198 - val_acc: 0.9936\n",
      "Epoch 7/50\n",
      " - 220s - loss: 0.0087 - acc: 0.9975 - val_loss: 0.0185 - val_acc: 0.9938\n",
      "Epoch 8/50\n",
      " - 218s - loss: 0.0072 - acc: 0.9978 - val_loss: 0.0226 - val_acc: 0.9937\n",
      "Epoch 9/50\n",
      " - 213s - loss: 0.0056 - acc: 0.9983 - val_loss: 0.0203 - val_acc: 0.9938\n"
     ]
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', patience=2)\n",
    "print('Beginning model {}.'.format('model 2'))\n",
    "model_2 = Sequential()\n",
    "model_2.add(Conv2D(32, kernel_size=(3, 3),activation='relu', input_shape=input_shape))\n",
    "model_2.add(BatchNormalization(axis=-1))\n",
    "model_2.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "model_2.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model_2.add(BatchNormalization(axis=-1))\n",
    "model_2.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model_2.add(BatchNormalization(axis=-1))\n",
    "model_2.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model_2.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model_2.add(BatchNormalization(axis=-1))\n",
    "model_2.add(Dropout(0.10))\n",
    "model_2.add(Flatten())\n",
    "\n",
    "model_2.add(Dense(512, activation='relu'))\n",
    "model_2.add(BatchNormalization(axis=-1))\n",
    "model_2.add(Dropout(0.35))\n",
    "#model_2.add(Dropout(0.20))\n",
    "model_2.add(Dense(num_classes, activation='softmax'))\n",
    "model_2.compile(loss='categorical_crossentropy', optimizer='Adadelta', metrics=['accuracy'])\n",
    "model_2.summary()\n",
    "history_model2 = model_2.fit(x_train, y_train,\n",
    "        batch_size=batch_size,\n",
    "        verbose=2,\n",
    "        epochs=50,\n",
    "        validation_data=(x_test, y_test),\n",
    "        callbacks=[early_stopping])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting weights and outputs from just the trained ConV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_name = 'flatten_2'\n",
    "intermediate_layer_model = Model(inputs=model_2.input,\n",
    "                                 outputs=model_2.get_layer(layer_name).output)\n",
    "intermediate_output = intermediate_layer_model.predict(x_train)\n",
    "intermediate_output_test = intermediate_layer_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Saving data\n",
    "with open( \"MNIST.pickle\", \"wb\" ) as file:\n",
    "    pickle.dump(( intermediate_output, intermediate_output_test), file)\n",
    "#Loading data\n",
    "with open( \"MNIST.pickle\", \"rb\" ) as file:\n",
    "    (intermediate_output, intermediate_output_test) = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the outputs from ConV for training and optimizing Supported Vector Machine (SVM) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'gamma': [0.00048828125, 0.0009765625, 0.001953125], 'C': [0.5, 1, 2, 5]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_train_digit = np.array([np.argmax(i) for i in y_train])\n",
    "y_test_digit = np.array([np.argmax(i) for i in y_test])\n",
    "\n",
    "parameters = {'gamma':[1/2048, 1/1024, 1/512], 'C':[0.5, 1, 2, 5]}\n",
    "svc = svm.SVC()\n",
    "clf2 = GridSearchCV(estimator = svc, param_grid = parameters, scoring = 'accuracy')\n",
    "clf2.fit(intermediate_output, y_train_digit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAg0AAAGACAYAAAAu3lEPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEBoAAIBbCA0AAMAt/w/ArhWRPH37KAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x43464f28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "C = np.repeat([0.5, 1, 2, 5],3)\n",
    "gamma = np.tile(['1/2048', '1/1024', '1/512'],4)\n",
    "\n",
    "results = pd.DataFrame({'C':C, 'gamma':gamma, 'acc':clf2.cv_results_['mean_test_score']})\n",
    "results = results.pivot('C', 'gamma', 'acc')\n",
    "f, ax = plt.subplots(figsize=(9, 6))\n",
    "sns.heatmap(results, linewidths=.5, ax=ax, cmap=\"YlGnBu\")\n",
    "plt.title('Mean test accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf2.predict(intermediate_output_test)\n",
    "acc = accuracy_score(y_pred, y_test_digit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy of the final model (ConV --> SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99509999999999998"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal 3: Mofifying the learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning model model 3.\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.1631 - acc: 0.9498       Learning rate:  1.0\n",
      "60000/60000 [==============================] - 188s 3ms/step - loss: 0.1629 - acc: 0.9498 - val_loss: 0.6480 - val_acc: 0.7879\n",
      "Epoch 2/50\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0472 - acc: 0.9856       Learning rate:  1.00787\n",
      "60000/60000 [==============================] - 194s 3ms/step - loss: 0.0471 - acc: 0.9856 - val_loss: 0.0300 - val_acc: 0.9906\n",
      "Epoch 3/50\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0279 - acc: 0.9914       Learning rate:  0.897657\n",
      "60000/60000 [==============================] - 196s 3ms/step - loss: 0.0279 - acc: 0.9914 - val_loss: 0.0260 - val_acc: 0.9910\n",
      "Epoch 4/50\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0207 - acc: 0.9931       Learning rate:  0.880592\n",
      "60000/60000 [==============================] - 197s 3ms/step - loss: 0.0207 - acc: 0.9931 - val_loss: 0.0225 - val_acc: 0.9917\n",
      "Epoch 5/50\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0140 - acc: 0.9958       Learning rate:  0.874258\n",
      "60000/60000 [==============================] - 197s 3ms/step - loss: 0.0140 - acc: 0.9958 - val_loss: 0.0235 - val_acc: 0.9929\n",
      "Epoch 6/50\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0112 - acc: 0.9967       Learning rate:  0.868412\n",
      "60000/60000 [==============================] - 199s 3ms/step - loss: 0.0112 - acc: 0.9967 - val_loss: 0.0229 - val_acc: 0.9932\n",
      "Epoch 7/50\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0086 - acc: 0.9971       Learning rate:  0.865943\n",
      "60000/60000 [==============================] - 200s 3ms/step - loss: 0.0087 - acc: 0.9971 - val_loss: 0.0194 - val_acc: 0.9936\n",
      "Epoch 8/50\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0071 - acc: 0.9978       Learning rate:  0.863828\n",
      "60000/60000 [==============================] - 199s 3ms/step - loss: 0.0071 - acc: 0.9978 - val_loss: 0.0207 - val_acc: 0.9937\n",
      "Epoch 9/50\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0054 - acc: 0.9985       Learning rate:  0.862414\n",
      "60000/60000 [==============================] - 199s 3ms/step - loss: 0.0054 - acc: 0.9985 - val_loss: 0.0187 - val_acc: 0.9948\n",
      "Epoch 10/50\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0044 - acc: 0.9987       Learning rate:  0.860966\n",
      "60000/60000 [==============================] - 199s 3ms/step - loss: 0.0044 - acc: 0.9987 - val_loss: 0.0202 - val_acc: 0.9931\n",
      "Epoch 11/50\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0036 - acc: 0.9989       Learning rate:  0.860095\n",
      "60000/60000 [==============================] - 199s 3ms/step - loss: 0.0036 - acc: 0.9989 - val_loss: 0.0198 - val_acc: 0.9936\n",
      "Epoch 12/50\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0026 - acc: 0.9994       Learning rate:  0.859423\n",
      "60000/60000 [==============================] - 199s 3ms/step - loss: 0.0026 - acc: 0.9994 - val_loss: 0.0220 - val_acc: 0.9934\n"
     ]
    }
   ],
   "source": [
    "# define learning rate function\n",
    "\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "\n",
    "class LossHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        #sd.append(step_decay(len(self.losses)))\n",
    "        print('       Learning rate: ', K.eval(self.model.optimizer.lr))\n",
    "        #print('       Loss Check: ', self.losses[-1])\n",
    "\n",
    "\n",
    "def scheduler(epoch):\n",
    "    if epoch > 0:\n",
    "        eta0 = 1.0/np.exp(0.1551) #Using the previous model's initial loss, 1.0 is the default lr vlue for Adadelta.\n",
    "        eta1 = eta0*np.exp(history.losses[-1])\n",
    "    else:\n",
    "        eta1 = 1.0\n",
    "    return eta1\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
    "print('Beginning model {}.'.format('model 3'))\n",
    "model_3 = Sequential()\n",
    "model_3.add(Conv2D(32, kernel_size=(3, 3),activation='relu', input_shape=input_shape))\n",
    "model_3.add(BatchNormalization(axis=-1))\n",
    "model_3.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "model_3.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model_3.add(BatchNormalization(axis=-1))\n",
    "model_3.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model_3.add(BatchNormalization(axis=-1))\n",
    "model_3.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model_3.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model_3.add(BatchNormalization(axis=-1))\n",
    "model_3.add(Dropout(0.10))\n",
    "model_3.add(Flatten())\n",
    "\n",
    "model_3.add(Dense(512, activation='relu'))\n",
    "model_3.add(BatchNormalization(axis=-1))\n",
    "model_3.add(Dropout(0.35))\n",
    "#model_2.add(Dropout(0.20))\n",
    "model_3.add(Dense(num_classes, activation='softmax'))\n",
    "model_3.compile(loss='categorical_crossentropy', optimizer='Adadelta', metrics=['accuracy'])\n",
    "#history_model2 = model_2.fit(x_train, y_train,                                                                                                                                                                                                                                                                           \n",
    "#        batch_size=batch_size,\n",
    "#        verbose=2,\n",
    "#        epochs=50,\n",
    "#        validation_data=(x_test, y_test),\n",
    "#        callbacks=[early_stopping])\n",
    "\n",
    "\n",
    "lrate=LearningRateScheduler(scheduler)   \n",
    "history = LossHistory()\n",
    "history_model3 = model_3.fit(x_train, y_train, batch_size=batch_size, epochs=50, verbose=1, \\\n",
    "                             validation_data=(x_test, y_test), callbacks=[history, lrate, early_stopping])\n",
    "\n",
    "#print(history.losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 40s 4ms/step\n",
      "Test loss: 0.0220352565714\n",
      "Test accuracy: 0.9934\n"
     ]
    }
   ],
   "source": [
    "# Score trained model.\n",
    "scores = model_3.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
